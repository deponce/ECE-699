{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea80f531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "import torchattacks\n",
    "from torchsummary import summary\n",
    "from torchvision import models, datasets, transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa597e3",
   "metadata": {},
   "source": [
    "import sys\n",
    "from watermarkJPEG import *\n",
    "from JPEG_layer import * # some utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccce171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "        [\n",
    "            #transforms.Resize(256),\n",
    "            #transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dataset = torchvision.datasets.ImageNet(root=\"./data/\", split='val',\n",
    "                             transform=transform\n",
    "                                       )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c4b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interpolate(nn.Module):\n",
    "    def __init__(self,size=(224,224), mode='bilinear') -> None:\n",
    "        super(Interpolate, self).__init__()\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        #input = torch.squeeze(input)\n",
    "        #return torch.unsqueeze(F.interpolate(input, size=self.size, mode=self.mode), dim=0)\n",
    "        return F.interpolate(input, size=self.size, mode=self.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49bab7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelIntegration(pretrained_model):\n",
    "    class IntegratedModel(nn.Module):\n",
    "        def __init__(self, pretrained_model):\n",
    "            super(IntegratedModel, self).__init__()\n",
    "            interpolate = Interpolate()\n",
    "            self.Integrated_Model = nn.Sequential(\n",
    "                interpolate,\n",
    "                pretrained_model\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.Integrated_Model(x)\n",
    "            return x\n",
    "    return IntegratedModel(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e69c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = models.resnet18(pretrained=True,  progress = True).to(device)\n",
    "#_ = pretrained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a815b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IntegratedModel = modelIntegration(pretrained_model).to(device)\n",
    "_=IntegratedModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ccec02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_image(im_as_var):\n",
    "    \"\"\"\n",
    "        Recreates images from a torch variable, sort of reverse preprocessing\n",
    "\n",
    "    Args:\n",
    "        im_as_var (torch variable): Image to recreate\n",
    "\n",
    "    returns:\n",
    "        recreated_im (numpy arr): Recreated image in array\n",
    "    \"\"\"\n",
    "    reverse_mean = [-0.485, -0.456, -0.406]\n",
    "    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n",
    "    recreated_im = torch.clone(im_as_var)\n",
    "    recreated_im = recreated_im.cpu().numpy()[0]\n",
    "    for c in range(3):\n",
    "        recreated_im[c] /= reverse_std[c]\n",
    "        recreated_im[c] -= reverse_mean[c]\n",
    "    recreated_im[recreated_im > 1] = 1\n",
    "    recreated_im[recreated_im < 0] = 0\n",
    "    #recreated_im = np.round(recreated_im * 255)\n",
    "    recreated_im = recreated_im.transpose(1, 2, 0)\n",
    "    #recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n",
    "    # Convert RBG to GBR\n",
    "    #recreated_im = recreated_im[..., ::-1]\n",
    "    return recreated_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0219a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_image(im_as_var):\n",
    "    recreated_im = torch.clone(im_as_var)\n",
    "    recreated_im = recreated_im.cpu().numpy()[0]\n",
    "    #recreated_im = np.round(recreated_im * 255)\n",
    "    recreated_im = recreated_im.transpose(1, 2, 0)\n",
    "    #recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n",
    "    # Convert RBG to GBR\n",
    "    #recreated_im = recreated_im[..., ::-1]\n",
    "    return recreated_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "100bb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = torchattacks.attacks.pgd.PGD(IntegratedModel,\n",
    "                                      eps=8/255,\n",
    "                                      alpha=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6631028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blockify(im: torch.Tensor, size: int) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Breaks an image into non-overlapping blocks of equal size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im : Tensor\n",
    "        The image to break into blocks, must be in :math:`(N, C, H, W)` format.\n",
    "    size : Tuple[int, int]\n",
    "        The size of the blocks in :math:`(H, W)` format.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tensor containing the non-overlappng blocks in :math:`(N, C, L, H, W)` format where :math:`L` is the\n",
    "    number of non-overlapping blocks in the image channel indexed by :math:`(N, C)` and :math:`(H, W)` matches\n",
    "    the block size.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    If the image does not split evenly into blocks of the given size, the result will have some overlap. It\n",
    "    is the callers responsibility to pad the input to a multiple of the block size, no error will be thrown\n",
    "    in this case.\n",
    "    \"\"\"\n",
    "    bs = im.shape[0]\n",
    "    ch = im.shape[1]\n",
    "    h = im.shape[2]\n",
    "    w = im.shape[3]\n",
    "\n",
    "    im = im.reshape(bs * ch, 1, h, w)\n",
    "    im = torch.nn.functional.unfold(im, kernel_size=(size, size), stride=(size, size))\n",
    "    im = im.transpose(1, 2)\n",
    "    im = im.reshape(bs, ch, -1, size, size)\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24fa8d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(N: int) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Computes the constant scale factor which makes the DCT orthonormal\n",
    "    \"\"\"\n",
    "    n = torch.ones((N, 1))\n",
    "    n[0, 0] = 1 / math.sqrt(2)\n",
    "    return n @ n.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1f31cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _harmonics(N: int) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Computes the cosine harmonics for the DCT transform\n",
    "    \"\"\"\n",
    "    spatial = torch.arange(float(N)).reshape((N, 1))\n",
    "    spectral = torch.arange(float(N)).reshape((1, N))\n",
    "\n",
    "    spatial = 2 * spatial + 1\n",
    "    spectral = (spectral * math.pi) / (2 * N)\n",
    "\n",
    "    return torch.cos(spatial @ spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8896bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_dct(blocks: torch.Tensor) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Computes the DCT of image blocks\n",
    "\n",
    "    Args:\n",
    "        blocks (Tensor): Non-overlapping blocks to perform the DCT on in :math:`(N, C, L, H, W)` format.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: The DCT coefficients of each block in the same shape as the input.\n",
    "\n",
    "    Note:\n",
    "        The function computes the forward DCT on each block given by \n",
    "\n",
    "        .. math::\n",
    "\n",
    "            D_{i,j}={\\frac {1}{\\sqrt{2N}}}\\alpha (i)\\alpha (j)\\sum _{x=0}^{N}\\sum _{y=0}^{N}I_{x,y}\\cos \\left[{\\frac {(2x+1)i\\pi }{2N}}\\right]\\cos \\left[{\\frac {(2y+1)j\\pi }{2N}}\\right]\n",
    "        \n",
    "        Where :math:`i,j` are the spatial frequency indices, :math:`N` is the block size and :math:`I` is the image with pixel positions :math:`x, y`. \n",
    "        \n",
    "        :math:`\\alpha` is a scale factor which ensures the transform is orthonormal given by \n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\alpha(u) = \\begin{cases}{\n",
    "                    \\frac{1}{\\sqrt{2}}} &{\\text{if }}u=0 \\\\\n",
    "                    1 &{\\text{otherwise}}\n",
    "                \\end{cases}\n",
    "        \n",
    "        There is technically no restriction on the range of pixel values but to match JPEG it is recommended to use the range [-128, 127].\n",
    "    \"\"\"\n",
    "    N = blocks.shape[3]\n",
    "\n",
    "    n = _normalize(N).float()\n",
    "    h = _harmonics(N).float()\n",
    "\n",
    "    if blocks.is_cuda:\n",
    "        n = n.cuda()\n",
    "        h = h.cuda()\n",
    "    \n",
    "    coeff = (1 / math.sqrt(2 * N)) * n * (h.t() @ blocks @ h)\n",
    "\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8747f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_ycbcr(image: torch.Tensor) -> torch.Tensor:\n",
    "    r\"\"\"Convert an RGB image to YCbCr.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): RGB Image to be converted to YCbCr.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: YCbCr version of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    if not torch.is_tensor(image):\n",
    "        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n",
    "            type(image)))\n",
    "\n",
    "    if len(image.shape) < 3 or image.shape[-3] != 3:\n",
    "        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n",
    "                         .format(image.shape))\n",
    "\n",
    "    r: torch.Tensor = image[..., 0, :, :]\n",
    "    g: torch.Tensor = image[..., 1, :, :]\n",
    "    b: torch.Tensor = image[..., 2, :, :]\n",
    "\n",
    "    delta = .5\n",
    "    y: torch.Tensor = .299 * r + .587 * g + .114 * b\n",
    "    cb: torch.Tensor = (b - y) * .564 + delta\n",
    "    cr: torch.Tensor = (r - y) * .713 + delta\n",
    "    return torch.stack((y, cb, cr), -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0bba3310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/50000 [00:29<40:32:51,  2.92s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1285.3757,  853.8616,  727.3779,  571.3262,  446.0864,  423.0541,\n",
       "          358.6108,  266.9424],\n",
       "        [ 862.3959,  730.6536,  643.5001,  508.7614,  407.0836,  380.1028,\n",
       "          321.3410,  242.9688],\n",
       "        [ 733.6719,  646.3253,  560.1807,  455.8930,  367.6834,  340.5405,\n",
       "          290.7578,  226.7159],\n",
       "        [ 594.9268,  540.8038,  466.2228,  377.1771,  310.1926,  293.3833,\n",
       "          256.3384,  210.4578],\n",
       "        [ 467.8964,  431.9413,  378.3402,  311.0285,  266.3231,  251.2716,\n",
       "          226.5678,  197.8513],\n",
       "        [ 407.3125,  375.2373,  332.2826,  280.7406,  245.2440,  232.6831,\n",
       "          212.3874,  190.2050],\n",
       "        [ 350.6968,  323.6541,  290.4131,  254.4960,  226.7369,  216.0959,\n",
       "          199.6210,  183.3576],\n",
       "        [ 301.0378,  271.9758,  251.7820,  228.7316,  209.4221,  200.5654,\n",
       "          189.5780,  178.0936]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_cnt = 0\n",
    "block_cnt = 0\n",
    "cum_turb = torch.zeros((8,8))\n",
    "for _ in tqdm((range(len(test_loader)))):\n",
    "    data, target = next(iter(test_loader))\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    #data.requires_grad = True\n",
    "    output = IntegratedModel(data)\n",
    "    init_pred = output.max(1, keepdim=True)[1]\n",
    "    attack_img = attack(data, init_pred[0])\n",
    "    # we assume that thh accuracy of the model is 1\n",
    "    #print((attack_img - data).shape)\n",
    "    #turb_noise = (attack_img - data).cpu().numpy()[0].transpose(1, 2, 0)\n",
    "    \n",
    "    turb_noise = (attack_img - data)[0]\n",
    "    \n",
    "    #print('turb_noise.shape',turb_noise.shape)\n",
    "    turb_noise = rgb_to_ycbcr(turb_noise)\n",
    "    #print('turb_noise.shape',turb_noise.shape,torch.max(turb_noise),torch.min(turb_noise))\n",
    "    \n",
    "    blocks = blockify(torch.unsqueeze(torch.unsqueeze(turb_noise[0],dim=0),dim=0),8) # get the Y channel, then reshape it to [N,C,H,W]\n",
    "    dct_cof = block_dct(blocks)\n",
    "    dct_cof = torch.squeeze(dct_cof)\n",
    "    cum_turb += torch.sum(torch.abs(dct_cof),dim = 0)\n",
    "    #print(dct_cof.shape,img_Y.shape)\n",
    "    #print(cum_turb)\n",
    "    if _ == 10:\n",
    "        break\n",
    "cum_turb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a5305571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/deponce/Passport/ECE699'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a27bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
